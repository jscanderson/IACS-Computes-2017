{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature analysis with n-grams\n",
    "\n",
    "As you probably know from your English homeworks, comparing two works of fiction can be a very hard and time-consuming task.\n",
    "It would be much nicer if we could just have the computer do all the work.\n",
    "But how could that work?\n",
    "\n",
    "One simple idea is that an author's style is represented by which words (s)he uses, and in particular which words (s)he uses most.\n",
    "Words are also known as *unigrams*.\n",
    "This is in contrast to *bigrams*, which consist of two words, *trigrams* (three words), and so on.\n",
    "For instance, the sentence\n",
    "\n",
    "    John likes Mary and Peter\n",
    "    \n",
    "contains the unigrams\n",
    "\n",
    "    John, likes, Mary, and, Peter\n",
    "    \n",
    "the bigrams\n",
    "\n",
    "    John likes, likes Mary, Mary and, and Peter\n",
    "    \n",
    "and the trigrams\n",
    "\n",
    "    John likes Mary, likes Mary and, Mary and Peter\n",
    "    \n",
    "We could also have 4-grams, 5-grams, or 127-grams.\n",
    "Quite generally, a model that is based on words or sequences of words is called an *n-gram model*.\n",
    "So if we want to analyze an author's style in terms of their word usage, we are proposing a unigram model of stylistic analysis.\n",
    "\n",
    "But does a unigram model actually work?\n",
    "Well, let's put the idea to the test: we will compare William Shakespeare's *Hamlet* and Christopher Marlowe's *The Tragical History of Dr. Faustus* using this technique.\n",
    "If we find something interesting, then unigram models might be worthwhile after all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the files\n",
    "\n",
    "First we need to have *Hamlet* and *Faustus* in some digital format that we can feed into Python.\n",
    "We want this to be a plaintext format, i.e. the pure text without any layout.\n",
    "We can use Python to download those files from [Project Gutenberg](https://www.gutenberg.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"http://www.gutenberg.org/cache/epub/1524/pg1524.html\", \"hamlet.txt\")\n",
    "urllib.request.urlretrieve(\"http://www.gutenberg.org/cache/epub/811/pg811.txt\", \"faustus.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code above should have put two files in the folder you are running this notebook from: `hamlet.txt` and `faustus.txt`.\n",
    "Open them with a text editor, for example Notepad if you are using a Windows computer.\n",
    "Scroll up and down a bit to get a better idea of what the files look like.\n",
    "Write down a list of the things that stand out to you.\n",
    "In particular:\n",
    "\n",
    "1. Do the two files look the same, or are there major differences?\n",
    "1. Do the files just contain the text of the plays, or also additional information (check the top and bottom of each file carefully)?\n",
    "1. If we want just the words used by the protagonists of the plays, what changes need to made to the files?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning up the files\n",
    "\n",
    "## Analysis\n",
    "\n",
    "You should have noticed quite a few problems with the files, only some of which we can fix by hand.\n",
    "\n",
    "1. While `faustus.txt` is fairly easy to read, `hamlet.txt` is cluttered with all kinds of weird code like `<p id=\"id00057\">` and `<br/>`. That's because we downloaded a textfile for `faustus.txt`,  but an html-file for `hamlet.txt`. The expressions between `<` and `>` are html-markup, which is needed to display a file in a webbrowser.\n",
    "\n",
    "1. Both files start with information about Project Gutenberg, which we do not want.\n",
    "\n",
    "1. Both files have information at the end that is not part of the play. In `hamlet.txt`, it's just a disclaimer that the play is over, whereas `faustus.txt` is full of footnotes.\n",
    "\n",
    "1. In `faustus.txt`, the text is often interrupted by strings like `[17]`. Those are references to footnotes.\n",
    "\n",
    "1. The files use different formats to indicate who is speaking.\n",
    "    - In `hamlet.txt`, names are abbreviated and occur between the markup `<p id=\"id...\">` and `<br/>`.\n",
    "    - In `faustus.txt`, names are fully capitalized.\n",
    "    \n",
    "1. Both files put stage instructions between square brackets, for example `[Francisco at his post. Enter to him Bernardo.]`.\n",
    "\n",
    "1. In `faustus.txt`, stage instructions are also indicated by indentation.\n",
    "\n",
    "1. In `faustus.txt`, all dialog is indented, but less so than the stage instructions.\n",
    "    \n",
    "1. Both files contain many empty lines.\n",
    "\n",
    "1. Both files capitalize words at the beginning of a new line.\n",
    "\n",
    "These are all problematic for us:\n",
    "\n",
    "- We just want to be able to see which words are used in each play, and how often each word is used.\n",
    "- We do not want HTML markup, information about Project Gutenberg, footnotes, or empty lines.\n",
    "- We also do not want to keep track of names if they just indicate who is speaking. That's not part of the play as such.\n",
    "- We should also exclude stage instructions because those do not belong to the literary part of the play either.\n",
    "\n",
    "Fixing all these things by hand would be tons of work.\n",
    "Fortunately, we only need to delete a few things by hand, while Python can do the rest.\n",
    "\n",
    "## Clean-up\n",
    "\n",
    "Let's first do the manual fixes:\n",
    "\n",
    "1. Open `hamlet.txt` and delete the first 362 lines. That's everything before the line `<h5 id=\"id00059\">SCENE. Elsinore.</h5>`gt\n",
    "\n",
    "1. Now go to the end of `hamlet.txt` and delete the last 7 lines. That's everything after and including the line `<p id=\"id01465\"> style=\"margin-top: 5em\">The End of Project Gutenberg Etext of Hamlet by Shakespeare<br />`.\n",
    "\n",
    "1. Open `faustus.txt` and delete the first 139 lines. That's everything up to and including the line `FROM THE QUARTO OF 1616.`\n",
    "\n",
    "1. In the same file, delete everything after the line `Terminat hora diem; terminat auctor opus.`\n",
    "   Use the editor's search function to find it quickly.\n",
    "\n",
    "Alright, now we have removed quite a bit of unwanted stuff from the files, but there's still many problems with the formatting.\n",
    "The Python code below will fix all of those for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# regular expressions; still to be done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the code, open the files `hamlet_clean.txt` and `faustus_clean.txt` in your text editor.\n",
    "All the unwanted annotations, markup and stage instructions are gone, and we have a much cleaner file now.\n",
    "Cleaning up files isn't too much fun, but it is really necessary.\n",
    "Always remember the old saying: **garbage in, garbage out!**\n",
    "We have to make sure our data is a clean as possible in order to do a good analysis.\n",
    "But now we can finally get started on the fun part!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Remember that we are interested in determining which words each author uses, and how often they do so.\n",
    "As far as Python is concerned, our text files are just a very long string of random characters.\n",
    "Python has no understanding of what a word is, so it cannot count words without our help.\n",
    "What we need to do is to tell Python how it can convert a string into a list of words.\n",
    "For example, the string\n",
    "\n",
    "    John likes John, Bill's mother, and Sue.\n",
    "\n",
    "should be converted to the list\n",
    "\n",
    "    [\"John\", \"likes\", \"John\", \",\", \"Bill\", \"'s\", \"mother\", \",\", \"and\", \"Sue\", \".\"]\n",
    "    \n",
    "Notice that the list may contain duplicates --- reading the list from left to right must give us the original sentence without any omissions.\n",
    "Also, punctuation is still included, but each punctuation mark is treated like an individual word.\n",
    "    \n",
    "This process of converting a string to a list is called *tokenization*.\n",
    "A *tokenizer* is a function that reads in a string and returns the corresponding list.\n",
    "So your first job is to write a function that reads in a whole play as a string and then returns the tokenized list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A few hints for writing a tokenizer:\n",
    "#\n",
    "# 1) You need to load the file first obviously;\n",
    "#    but I suggest that you also take a look at the corresponding strings;\n",
    "#    you will encounter a few surprises, in particular many instances of \\n\n",
    "#\n",
    "# 2) For the purpose of tokenization, the following things are words:\n",
    "#    - punctuation symbols (! ? . , ; :)\n",
    "#    - a hyphen (-) if it is surrounded by white space; but in words like laurel-bough, the - is not a word\n",
    "#    - 's, both as a possessive marker (John's mother) and as a shortened form of is (it's, there's)\n",
    "#    - any string of characters between whitespace or punctuation symbols\n",
    "#    - whitespace refers to a space, but also to the special characters \\t (tab) and \\n (new line)\n",
    "#\n",
    "# 3) I suggest you iterate over the positions in the string with a for-loop.\n",
    "#    Save the last position that was a punctuation symbol or whitespace.\n",
    "#    If you encounter another punctuation symbol or whitespace,\n",
    "#    the string between this current position and the previously saved one is a word.\n",
    "#    But careful: the apostrophe complicates things a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting the tokenized list to a dictionary\n",
    "\n",
    "A tokenized list is nice, but not enough.\n",
    "We also want to know how often each word is used.\n",
    "We can do this with a dictionary where the keys are words and the values indicate how often the word occurs in the tokenized list.\n",
    "For example, the list\n",
    "\n",
    "    [\"John\", \"likes\", \"John\", \",\", \"Bill\", \"'s\", \"mother\", \",\", \"and\", \"Sue\", \".\"]\n",
    "    \n",
    "should be converted to the dictionary\n",
    "\n",
    "    {\"John\": 2, \"likes\": 1, \",\": 2, \"Bill\": 1, \"'s\": 1, \"mother\": 1, \"and\": 1, \"Sue\": 1, \".\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a function that converts the tokenized lists to dictionaries\n",
    "# \n",
    "# Pro-tip: Python has a special subtype of dictionaries called Counters, which are perfect for this task.\n",
    "#          So if you want, you can use a Counter instead of a dictionary.\n",
    "#          Google around a bit to see how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the dictionaries\n",
    "\n",
    "## Our first attempt...\n",
    "\n",
    "Alright, now we finally have word counts for both plays.\n",
    "But what are we supposed to do with them?\n",
    "The dictionaries are way too large to compare by hand.\n",
    "Well, we could first compare the ten most common words in each dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here's a nice helper function\n",
    "def dict_to_ordered_list(dictionary):\n",
    "    \"\"\"convert dictionary to list of keys, ordered by decreasing value\"\"\"\n",
    "    # key=dictionary.get uses the values for sorting\n",
    "    # reverse=True sorts the list in descending order rather than ascending\n",
    "    return sorted(dictionary, key=dictionary.get, reverse=True)\n",
    "\n",
    "# Your turn! Use the helper function to look at the ten most common words in each play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ...is a failure\n",
    "\n",
    "Well that output isn't very helpful, it's all just useless words like *a* and *the* and *to*.\n",
    "But that's actually expected.\n",
    "Remember what you learned about **Zipf's law**: a handful of words make up over 50% of all words in a text.\n",
    "Looking at the ten most common words in each play can't reveal much because Zipf's law already tells us that those words won't be interesting.\n",
    "If we want to find anything of interest, we have to ignore these common but uninteresting words, which are also called *stop words*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Okay, time to deal with stop words:\n",
    "#\n",
    "# 1) Download the file from\n",
    "#    https://raw.githubusercontent.com/stanfordnlp/CoreNLP/master/data/edu/stanford/nlp/patterns/surface/stopwords.txt\n",
    "# 2) Each line of the file contains a stop word.\n",
    "#    Read in the file and convert it to a list of stop words.\n",
    "# 3) Use this list to look at the ten most common words in each play that are not stop words.\n",
    "#\n",
    "#\n",
    "# Pro-tip: Step 3 can be done with a single line of code using a Python list comprehension;\n",
    "#          if you're curious, ask your tutor or google around a bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the data\n",
    "\n",
    "Now we have a rough idea which words are the most common in each play, and a few differences already show up at this point.\n",
    "But that's not the only thing we could look at.\n",
    "Maybe one author is much less creative in their choice of words than the other.\n",
    "Then we would expect that the uncreative author uses the same words over and over, whereas the creative one avoids repetitions.\n",
    "\n",
    "We can rephrase this idea as a problem of relative frequency: how frequent are words relative to other words in the text?\n",
    "And this can be represented as a graph where words are placed along the x-axis and the y-axis indicates for each word how often it occurs.\n",
    "When the words are ordered by frequency, a creative author should produce a graph with a very long tail --- that is to say, there's many, many words with very low frequency.\n",
    "An uncreative author, on the other hand, should have a small stock of words that occur over and over and a comparatively shorter tail.\n",
    "\n",
    "We can use Python's *matplotlib* package to produce such graphs for us.\n",
    "Matplotlib will take the dictionaries as input with keys for the x-axis and values for the y-axis.\n",
    "But recall that our dictionaries still contain the unwanted stop words.\n",
    "We need to fix this first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dict_without_stopwords(dictionary, stopword_list):\n",
    "    \"\"\"remove stop words from dictionary\"\"\"\n",
    "    # some magic happens here\n",
    "    return clean_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally look at the graphs for *Hamlet* and *Dr. Faustus*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# matplotlib code; can the students do it on their own?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on those graphs, is Shakespeare or Marlowe the more creative writer?\n",
    "Is the result surprising to you?\n",
    "Did we make mistakes in our analysis?\n",
    "If so, what changes should we make?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving to bigrams and trigrams\n",
    "\n",
    "One could argue that just counting words is not an adequate representation of a writer's style.\n",
    "Often the important thing is not so much which words an author uses, but how they use them.\n",
    "While a sophisicated analysis of writing style is a very difficult affair, our unigram approach can be easily tweaked to move beyond words.\n",
    "\n",
    "One way of formalizing the idea of *creative word usage* is to look in which order words appear.\n",
    "That is very easy with n-grams.\n",
    "From the list of tokenized words, one can easily construct a list of n-grams.\n",
    "For example, the list\n",
    "\n",
    "    [\"John\", \"likes\", \"John\", \",\", \"Bill\", \"'s\", \"mother\", \",\", \"and\", \"Sue\", \".\"]\n",
    "    \n",
    "could be converted into lists of bigrams.\n",
    "\n",
    "    [(\"John\", \"likes\"),\n",
    "     (\"likes\", \"John\"),\n",
    "     (\"John\", \",\"),\n",
    "     (\",\", \"Bill\"),\n",
    "     (\"Bill\", \"'s\"),\n",
    "     (\"'s\", \"mother\"),\n",
    "     (\",\", \"and\"),\n",
    "     (\"and\", \"Sue\"),\n",
    "     (\"Sue\", \".\")]\n",
    "\n",
    "Note that we treat n-grams as tuples in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_to_ngrams(tokenized_list, n):\n",
    "    \"\"\"convert tokenized list to list of n-grams (n = 2 or greater)\"\"\"\n",
    "    # some magic happens here\n",
    "    return ngram_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can repeat our previous analysis using bigrams or trigrams instead of just words.\n",
    "(We could also look at 4-grams or 5-grams, but that is a lot harder to compute and we just don't have enough data to use such large n-grams effectively.)\n",
    "So let's see if the step from words to bigrams/trigrams changes things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute dictionaries to keep track of how often each n-gram occurs.\n",
    "# You do not need to worry about stop words in this case.\n",
    "\n",
    "\n",
    "# Plot the data with matplotlib and see if the frequencies change a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now it is time for you to formulate a final verdict:\n",
    "\n",
    "- Has the move to bigrams/trigrams changed the results a lot?\n",
    "- Do those results reflect your personal expectations about the author's writing styles?\n",
    "- Depending on your answer, how much would you trust such quantiative approaches to evaluating writing style?\n",
    "- Would it surprise you to hear that more and more colleges across the US are relying on such software to grade student essays?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
