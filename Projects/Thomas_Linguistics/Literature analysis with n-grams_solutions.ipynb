{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature analysis with n-grams\n",
    "\n",
    "As you probably know from your English homeworks, comparing works of fiction can be a very hard and time-consuming task.\n",
    "It would be much nicer if we could just have the computer do all the work.\n",
    "But how could that work?\n",
    "\n",
    "One simple idea is that an author's style is represented by which words (s)he uses, and in particular which words (s)he uses most.\n",
    "Words are also known as *unigrams*.\n",
    "This is in contrast to *bigrams*, which consist of two words, *trigrams* (three words), and so on.\n",
    "For instance, the sentence\n",
    "\n",
    "    John likes Mary and Peter\n",
    "    \n",
    "contains the unigrams\n",
    "\n",
    "    John, likes, Mary, and, Peter\n",
    "    \n",
    "the bigrams\n",
    "\n",
    "    John likes, likes Mary, Mary and, and Peter\n",
    "    \n",
    "and the trigrams\n",
    "\n",
    "    John likes Mary, likes Mary and, Mary and Peter\n",
    "    \n",
    "We could also have 4-grams, 5-grams, or 127-grams.\n",
    "Quite generally, a model that is based on words or sequences of words is called an *n-gram model*.\n",
    "So if we want to analyze an author's style in terms of their word usage, we are proposing a unigram model of stylistic analysis.\n",
    "\n",
    "But does a unigram model actually work?\n",
    "Well, let's put the idea to the test: we will compare three works of fiction comparing this technique:\n",
    "\n",
    "- William Shakespeare's *Hamlet*\n",
    "- Christopher Marlowe's *The Tragical History of Dr. Faustus*\n",
    "- Edgar Rice Burrough's *A Princess of Mars*\n",
    "\n",
    "If we find something interesting, then unigram models might be worthwhile after all.\n",
    "\n",
    "A brief remark on those works: The first two are world-famous plays, whereas the third is an early 20th century pulp novel that you might know as the basis for Disney's 2012 box office debacle *John Carter*. Although the movie is better than its reputation, it still doesn't do justice to the book, so give it a read if you are in the mood for a fun science fantasy read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the files\n",
    "\n",
    "First we need to have the books in some digital format that we can feed into Python.\n",
    "Ideally, we want this to be a plaintext format, i.e. the pure text without any layout information.\n",
    "We do not want a pdf or doc file, as those are much harder to work with.\n",
    "We can use Python to download all the files from [Project Gutenberg](https://www.gutenberg.org/), an online platform that hosts literary works that are no longer under copyright."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mars.txt', <http.client.HTTPMessage at 0x10e002668>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"http://www.gutenberg.org/cache/epub/1524/pg1524.html\", \"hamlet.txt\")\n",
    "urllib.request.urlretrieve(\"http://www.gutenberg.org/cache/epub/811/pg811.txt\", \"faustus.txt\")\n",
    "urllib.request.urlretrieve(\"http://www.gutenberg.org/cache/epub/62/pg62.txt\", \"mars.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code above should have put three files in the folder you are running this notebook from:\n",
    "\n",
    "1.  `faustus.txt`\n",
    "1.  `hamlet.txt`\n",
    "1.  `mars.txt`\n",
    "\n",
    "Open them with a text editor, for example Notepad if you are using a Windows computer.\n",
    "Scroll up and down a bit to get a better idea of what the files look like.\n",
    "Write down a list of the things that stand out to you.\n",
    "In particular:\n",
    "\n",
    "1. Do the two files look the same, or are there major differences?\n",
    "1. Do the files just contain the text of the plays, or also additional information (check the top and bottom of each file carefully)?\n",
    "1. If we want just the words used by the protagonists of the plays, what changes need to made to the files?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning up the files\n",
    "\n",
    "## Analysis\n",
    "\n",
    "You should have noticed quite a few problems with the files, only some of which we can fix by hand.\n",
    "\n",
    "1. While `faustus.txt` and `mars.txt` are fairly easy to read, `hamlet.txt` is cluttered with all kinds of weird code like `<p id=\"id00057\">` and `<br/>`. That's because we downloaded a textfile for `faustus.txt` and `mars.txt`, but an html-file for `hamlet.txt`. The expressions between `<` and `>` are html-markup, which is needed to display a file in a webbrowser.\n",
    "\n",
    "1. All files start with information about Project Gutenberg, which we do not want.\n",
    "\n",
    "1. All files have information at the end that is not part of the play. In `hamlet.txt`, it's just a disclaimer that the play is over, `mars.txt` ends with the Project Gutenberg license, and `faustus.txt` is full of footnotes.\n",
    "\n",
    "1. In `faustus.txt`, the text is often interrupted by strings like `[17]`. Those are references to footnotes.\n",
    "\n",
    "1. For the two plays, different formats are used to indicate who is speaking.\n",
    "    - In `hamlet.txt`, names are abbreviated and occur between the markup `<p id=\"id...\">` and `<br/>`.\n",
    "    - In `faustus.txt`, names are fully capitalized.\n",
    "    \n",
    "1. Both plays put stage instructions between square brackets, for example `[Francisco at his post. Enter to him Bernardo.]`.\n",
    "\n",
    "1. In `faustus.txt`, stage instructions are also indicated by indentation.\n",
    "\n",
    "1. In `faustus.txt`, all dialog is indented, but less so than the stage instructions.\n",
    "    \n",
    "1. All three files contain many empty lines.\n",
    "\n",
    "1. Both plays capitalize words at the beginning of a new line.\n",
    "\n",
    "1. In `mars.txt`, Chapters are written in upper caps.\n",
    "\n",
    "These are all problematic for us:\n",
    "\n",
    "- We just want to be able to see which words are used in each play, and how often each word is used.\n",
    "- We do not want HTML markup, information about Project Gutenberg, footnotes, or empty lines.\n",
    "- We also do not want to keep track of names if they just indicate who is speaking. That's not part of the play as such.\n",
    "- We should also exclude stage instructions because those do not belong to the literary part of the play either.\n",
    "\n",
    "Fixing all these things by hand would be tons of work.\n",
    "Fortunately, we only need to delete a few things by hand, while Python can do the rest.\n",
    "\n",
    "## Clean-up\n",
    "\n",
    "Let's first do the manual fixes:\n",
    "\n",
    "1. Open `hamlet.txt` and delete the first 362 lines. That's everything before the line `<h5 id=\"id00059\">SCENE. Elsinore.</h5>`gt\n",
    "\n",
    "1. Now go to the end of `hamlet.txt` and delete the last 7 lines. That's everything after and including the line `<p id=\"id01465\"> style=\"margin-top: 5em\">The End of Project Gutenberg Etext of Hamlet by Shakespeare<br />`.\n",
    "\n",
    "1. Open `faustus.txt` and delete the first 140 lines. That's everything up to and including the empty line right after `FROM THE QUARTO OF 1616.`\n",
    "\n",
    "1. In the same file, delete everything after the line `Terminat hora diem; terminat auctor opus.`\n",
    "   Use the editor's search function to find it quickly.\n",
    "   \n",
    "1. Open `mars.txt` and delte the first 235 lines. that's everything before the line that says `CHAPTER I`.\n",
    "\n",
    "1. In the same file, delete everything after the line `that I shall soon know.`\n",
    "\n",
    "Save the files under new names so that they don't get overwritten in case you redownload the files: `hamlet_manual`, `faustus_manual`, and `mars_manual`.\n",
    "We have removed quite a bit of unwanted stuff, but there's still many problems with the formatting.\n",
    "The Python code below fixes all of those for us using a powerful pattern matching technique called *regular expressions*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'hamlet_manual.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-545f34476b24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# do the actual cleaning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"hamlet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"faustus\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mars\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mtext_cleaner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-545f34476b24>\u001b[0m in \u001b[0;36mtext_cleaner\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \"\"\"\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Step 1: load file and store it as variable \"text\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_manual.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8-sig'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Step 2: create a new file to save cleaned up version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_clean.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcleaned\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hamlet_manual.txt'"
     ]
    }
   ],
   "source": [
    "# Code to clean up hamlet.txt, faustus.txt, and mars.txt\n",
    "# ======================================================\n",
    "\n",
    "# import regular expression module\n",
    "import re\n",
    "\n",
    "# define a general function for loading and writing files\n",
    "def text_cleaner(filename):\n",
    "    \"\"\"\n",
    "    Open text and run required cleaning procedures.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    filename: str\n",
    "        name of file without extension (for instance .txt)\n",
    "    \"\"\"\n",
    "    # Step 1: load file and store it as variable \"text\"\n",
    "    with open(filename + \"_manual.txt\", mode=\"r\", encoding='utf-8-sig') as text:\n",
    "        # Step 2: create a new file to save cleaned up version\n",
    "        with open(filename + \"_clean.txt\", mode=\"w\", encoding='utf-8') as cleaned:\n",
    "            # Step 3: clean each line and write to clean-up file\n",
    "            for line in text:\n",
    "                # cleaning\n",
    "                line = line_cleaner(filename, line)\n",
    "                # write line if it isn't empty\n",
    "                if line != \"\\n\":\n",
    "                    cleaned.write(line)\n",
    "\n",
    "# define a function that cleans each line for each text\n",
    "def line_cleaner(filename, line):\n",
    "    \"\"\"clean line for hamlet, faustus, and mars\"\"\"\n",
    "    # hamlet-specific cleaning\n",
    "    if filename == \"hamlet\":\n",
    "        # 1. remove all headers\n",
    "        line = re.sub(r'<h[0-9].*', r'', line)\n",
    "        # 2. remove speaker information\n",
    "        #    (identified by html tags)\n",
    "        line = re.sub(r'<p id=\"id[0-9]*\">[^<]*<br/>', r'', line)\n",
    "        # 3. remove html tags\n",
    "        line = re.sub(r'<[^>]*>', r'', line)\n",
    "        # 4. remove anything after [ or before ]\n",
    "        line = re.sub(r'\\[[^\\]]*', r'', line)\n",
    "        line = re.sub(r'[^\\[]*\\]', r'', line)\n",
    "    # faustus-specific cleaning\n",
    "    elif filename == \"faustus\":\n",
    "        # 1. remove stage information\n",
    "        #    (anything after 10 spaces)\n",
    "        line = re.sub(r'(\\s){10}.*', r'', line)\n",
    "        # 2. remove speaker information\n",
    "        #    (any word in upper caps followed by space or dot)\n",
    "        line = re.sub(r'[A-Z]{2,}[\\s\\.]', r'', line)\n",
    "        # 3. remove anything between square brackets\n",
    "        line = re.sub(r'\\[[^\\]]*\\]', r'', line)\n",
    "        # 4. remove sentence initial spaces\n",
    "        line = re.sub(r'^\\s+', r'', line)\n",
    "    # mars-specific cleaning\n",
    "    elif filename == \"mars\":\n",
    "        # 1. delete CHAPTER I\n",
    "        # (must be done like this because Roman 1 looks like English I)\n",
    "        line = re.sub('CHAPTER I', '', line)\n",
    "        # 2. remove any word in upper caps\n",
    "        line = re.sub(r'[A-Z]{2,}[\\s\\.]', r'', line)\n",
    "        # 3. remove anything after [ or before ]\n",
    "        line = re.sub(r'\\[[^\\]]*', r'', line)\n",
    "        line = re.sub(r'[^\\[]*\\]', r'', line)\n",
    "    else:\n",
    "        raise Exception(\"No cleaning profile exists for this file\")\n",
    "    # remove multiple spaces that might be left after clean up\n",
    "    line = re.sub(r'\\ +', ' ', line)\n",
    "    # return cleaned up line with everything in lower case\n",
    "    return line.lower()\n",
    "        \n",
    "# do the actual cleaning\n",
    "for filename in [\"hamlet\", \"faustus\", \"mars\"]:\n",
    "    text_cleaner(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the code, open the files `faustus_clean.txt`, `hamlet_clean.txt`, and `mars_clean.txt` in your text editor.\n",
    "All the unwanted annotations, markup and stage instructions are gone, and we have a much cleaner file now.\n",
    "Also note that now all words are lowercase, including proper names.\n",
    "That is a feature, not a bug: *but* and *But* are the same word, so we do not want to count them separately.\n",
    "That the texts now talk about *hamlet*, *faustus*, and *carter* is not much of an issue since proper names are rarely identical to existing words.\n",
    "\n",
    "Cleaning up files isn't too much fun, but it is really necessary.\n",
    "Always remember the old saying: **garbage in, garbage out!**\n",
    "We have to make sure our data is a clean as possible in order to do a good analysis.\n",
    "But now we can finally get started on the fun part!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Remember that we are interested in determining which words each author uses, and how often they do so.\n",
    "As far as Python is concerned, our text files are just a very long string of random characters.\n",
    "Python has no understanding of what a word is, so it cannot count words without our help.\n",
    "What we need to do is to tell Python how it can convert a string into a list of words.\n",
    "For example, the string\n",
    "\n",
    "    John likes John, Bill's mother, and Sue.\n",
    "\n",
    "should be converted to the list\n",
    "\n",
    "    [\"John\", \"likes\", \"John\", \",\", \"Bill\", \"'s\", \"mother\", \",\", \"and\", \"Sue\", \".\"]\n",
    "    \n",
    "Notice that the list may contain duplicates --- reading the list from left to right must give us the original sentence without any omissions.\n",
    "Also, punctuation is still included, but each punctuation mark is treated like an individual word.\n",
    "    \n",
    "This process of converting a string to a list is called *tokenization*.\n",
    "A *tokenizer* is a function that reads in a string and returns the corresponding list.\n",
    "So your first job is to write a function that reads in a whole play as a string and then returns the tokenized list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few hints for writing a tokenizer:\n",
    "#\n",
    "# 1) You need to load the file first obviously;\n",
    "#    but I suggest that you also take a look at the corresponding strings;\n",
    "#    you will encounter a few surprises, in particular many instances of \\n;\n",
    "#    each \\n marks the end of a line\n",
    "#\n",
    "# 2) For the purpose of tokenization, the following things are words:\n",
    "#    - punctuation symbols (! ? . , ; :)\n",
    "#    - a hyphen (-) if it is preceded by whitespace; but in words like laurel-bough, the - is not a word\n",
    "#    - 's, both as a possessive marker (John's mother) and as a shortened form of is (it's, there's)\n",
    "#    - any string of characters between whitespace or punctuation symbols\n",
    "#    - whitespace refers to a space, but also to the special characters \\t (tab) and \\n (new line)\n",
    "#\n",
    "# 3) I suggest you iterate over the positions in the string with a for-loop.\n",
    "#    Save the last position that was a punctuation symbol or whitespace.\n",
    "#    If you encounter another punctuation symbol or whitespace,\n",
    "#    the string between this current position and the previously saved one is a word.\n",
    "#    But careful: the apostrophe complicates things a bit.\n",
    "#\n",
    "#    Alternatively, you can experiment with Python's .split method for strings,\n",
    "#    if you know how to use it.\n",
    "#\n",
    "# Extra Challenge: The English apostrophe also shows up in contractions of will ('ll) and not (n't).\n",
    "# Adapt your code so that it tokenizes \"I'll\" as \"I will\", \"hasn't\" as \"has not\", and similarly for related instances.\n",
    "# The quality of your original solution shows in how easily it can be adapted to these new problems.\n",
    "\n",
    "punctuation = [\"!\", \"?\", \".\", \",\", \";\", \":\"]\n",
    "whitespace = [\" \", \"\\n\", \"\\t\"]\n",
    "    \n",
    "def tokenizer(filename):\n",
    "    \"\"\"tokenize text given by filename\"\"\"\n",
    "\n",
    "    \n",
    "    # read in file to start tokenizing\n",
    "    with open(filename, \"r\") as text:\n",
    "        text = text.read()\n",
    "        # do not tokenize empty texts\n",
    "        if len(text) == 0:\n",
    "            return []\n",
    "    \n",
    "        # treat position 0 separately so that we can have a sane loop\n",
    "        # that looks at preceding elements without running out of index\n",
    "        if text[0] in whitespace:\n",
    "            tokens = [\"\"]\n",
    "        else:\n",
    "            tokens = [text[0]]\n",
    "        \n",
    "        # now iterate over all positions starting from 1\n",
    "        for pos in range(1, len(text)):\n",
    "            char = text[pos]\n",
    "            # punctuation is a new token;\n",
    "            # we assume that\n",
    "            # 1) punctuation is never preceded by whitespace, and\n",
    "            # 2) punctuation cannot be part of a larger word\n",
    "            if char in punctuation:\n",
    "                tokens.append(char)\n",
    "                tokens.append(\"\")\n",
    "            # new whitespace adds \"\" to mark potential start of new word\n",
    "            elif char in whitespace:\n",
    "                if tokens[-1] != \"\":\n",
    "                    tokens.append(\"\")\n",
    "            # for s, check whether it is part of 's\n",
    "            elif char == \"s\" and text[pos-1] == \"'\":\n",
    "                tokens.append(text[pos-1:pos+1])\n",
    "            # now check whether hyphen is preceded by whitespace\n",
    "            elif char == \"-\" and text[pos-1] in whitespace:\n",
    "                tokens.append(char)\n",
    "            # all other characters except ' and - get added to last item in tokens\n",
    "            elif char != \"'\":\n",
    "                tokens[-1] += char\n",
    "    return tokens\n",
    "\n",
    "hamlet = tokenizer(\"hamlet_clean.txt\")\n",
    "faustus = tokenizer(\"faustus_clean.txt\")\n",
    "mars = tokenizer(\"mars_clean.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting the tokenized list to a dictionary\n",
    "\n",
    "A tokenized list is nice, but not enough.\n",
    "We also want to know how often each word is used.\n",
    "We can do this with a dictionary where the keys are words and the values indicate how often the word occurs in the tokenized list.\n",
    "For example, the list\n",
    "\n",
    "    [\"John\", \"likes\", \"John\", \",\", \"Bill\", \"'s\", \"mother\", \",\", \"and\", \"Sue\", \".\"]\n",
    "    \n",
    "should be converted to the dictionary\n",
    "\n",
    "    {\"John\": 2, \"likes\": 1, \",\": 2, \"Bill\": 1, \"'s\": 1, \"mother\": 1, \"and\": 1, \"Sue\": 1, \".\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that converts the tokenized lists to dictionaries\n",
    "# \n",
    "# Pro-tip: Python has a special subtype of dictionaries called Counters, which are perfect for this task.\n",
    "#          So if you want, you can use a Counter instead of a dictionary.\n",
    "#          Google around a bit to see how they work.\n",
    "\n",
    "def ngram_counter(tokens):\n",
    "    counts = {}\n",
    "    for pos in range(len(tokens)):\n",
    "        token = tokens[pos]\n",
    "        if counts.get(token):\n",
    "            counts[token] += 1\n",
    "        else:\n",
    "            counts[token] = 1\n",
    "    return counts\n",
    "\n",
    "hamlet_counts = ngram_counter(hamlet)\n",
    "faustus_counts = ngram_counter(faustus)\n",
    "mars_counts = ngram_counter(mars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the dictionaries\n",
    "\n",
    "## Our first attempt...\n",
    "\n",
    "Alright, now we finally have word counts for both plays.\n",
    "But what are we supposed to do with them?\n",
    "The dictionaries are way too large to compare by hand.\n",
    "Well, we could first compare the ten most common words in each dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a nice helper function\n",
    "def dict_to_ordered_list(dictionary):\n",
    "    \"\"\"convert dictionary to list of keys, ordered by decreasing value\"\"\"\n",
    "    # key=dictionary.get uses the values for sorting\n",
    "    # reverse=True sorts the list in descending order rather than ascending\n",
    "    return sorted(dictionary, key=dictionary.get, reverse=True)\n",
    "\n",
    "# Your turn! Use the helper function to look at the ten most common words in each play.\n",
    "for text in [hamlet_counts, faustus_counts, mars_counts]:\n",
    "    print(dict_to_ordered_list(text)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ...is a failure\n",
    "\n",
    "Well that output isn't very helpful, it's all just punctuation and useless words like *a* and *the* and *to*.\n",
    "But that's actually expected.\n",
    "Punctuation is very common in written text.\n",
    "And remember what you learned about **Zipf's law**: a handful of words make up over 50% of all words in a text.\n",
    "Looking at the ten most common words in each play can't reveal much because Zipf's law already tells us that those words won't be interesting.\n",
    "If we want to find anything of interest, we have to ignore punctuation and all these common but uninteresting words, which are also called *stop words*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, time to deal with stop words:\n",
    "#\n",
    "# 1) Download the file from\n",
    "#    https://raw.githubusercontent.com/stanfordnlp/CoreNLP/master/data/edu/stanford/nlp/patterns/surface/stopwords.txt\n",
    "# 2) Each line of the file contains a stop word.\n",
    "#    Read in the file and convert it to a list of stop words.\n",
    "# 3) Use this list to look at the ten most common words in each play that are not stop words.\n",
    "#    If necessary, you can add stop words beyond what is provided in the list to filter out additional words.\n",
    "# 4) Don't forget to also filter out all punctuation.\n",
    "#\n",
    "#\n",
    "# Pro-tip: Step 3 can be done with a single line of code using a Python list comprehension;\n",
    "#          if you're curious, ask your tutor or google around a bit\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/stanfordnlp/CoreNLP/master/data/edu/stanford/nlp/patterns/surface/stopwords.txt\", \"stopwords.txt\")\n",
    "\n",
    "with open(\"stopwords.txt\") as stopwords:\n",
    "    stopwords = [line.strip('\\n') for line in stopwords]\n",
    "    print(stopwords)\n",
    "\n",
    "hamlet_good_counts = [word for word in dict_to_ordered_list(hamlet_counts)\n",
    "                      if word not in stopwords and word not in punctuation][:10]\n",
    "faustus_good_counts = [word for word in dict_to_ordered_list(faustus_counts)\n",
    "                      if word not in stopwords and word not in punctuation][:10]\n",
    "mars_good_counts = [word for word in dict_to_ordered_list(mars_counts)\n",
    "                      if word not in stopwords and word not in punctuation][:10]\n",
    "\n",
    "print(hamlet_good_counts)\n",
    "print(faustus_good_counts)\n",
    "print(mars_good_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the data\n",
    "\n",
    "Now we have a rough idea which words are the most common in each play, and a few differences already show up at this point.\n",
    "In particular, it should be pretty easy for you to tell for each list which book it was built from.\n",
    "Try experimenting with larger values and whether they make the lists more similar or more distinct.\n",
    "\n",
    "But we could look at much more than just the relative ranking of words.\n",
    "Maybe one author is much less creative in their choice of words than the other.\n",
    "Then we would expect that the uncreative author uses the same words over and over, whereas the creative one avoids repetitions.\n",
    "\n",
    "We can rephrase this idea as a problem of relative frequency: how frequent are words relative to other words in the text?\n",
    "And this can be represented as a graph where words are placed along the x-axis and the y-axis indicates for each word how often it occurs.\n",
    "When the words are ordered by frequency, a creative author should produce a graph with a very long tail --- that is to say, there's many, many words with very low frequency.\n",
    "An uncreative author, on the other hand, should have a small stock of words that occur over and over and a comparatively shorter tail.\n",
    "\n",
    "We can use Python's *matplotlib* package to produce such graphs for us.\n",
    "Matplotlib will take the dictionaries as input with keys for the x-axis and values for the y-axis.\n",
    "But recall that our dictionaries still contain the unwanted stop words and punctuation.\n",
    "We need to fix this first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_without_crud(dictionary, list_of_crud):\n",
    "    \"\"\"remove unwanted entries from dictionary\"\"\"\n",
    "    # some magic happens here\n",
    "    clean_dictionary = {key: dictionary[key] for key in dictionary.keys()\n",
    "                        if key not in list_of_crud}\n",
    "    return clean_dictionary\n",
    "\n",
    "hamlet_dict = dict_without_crud(hamlet_counts, punctuation + stopwords)\n",
    "faustus_dict = dict_without_crud(faustus_counts, punctuation + stopwords)\n",
    "mars_dict = dict_without_crud(mars_counts, punctuation + stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally look at the graphs for *Hamlet*, *Dr. Faustus*, and *Princess of Mars*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%pylab inline\n",
    "figsize(20, 10)\n",
    "# the lines above are needed for Jupyter to display the plots in your browser;\n",
    "# do not remove them\n",
    "\n",
    "# import relevant matplotlib code\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# a little bit of preprocessing so that the data is ordered by frequency\n",
    "def plot_preprocess(dictionary, n):\n",
    "    \"\"\"format data for plotting n most common items\"\"\"\n",
    "    sorted_list = sorted(dictionary.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "    words, counts = zip(*sorted_list)\n",
    "    return words, counts\n",
    "\n",
    "for dictionary in [hamlet_dict, faustus_dict, mars_dict]:\n",
    "    # you can change the max words value to look at more or fewer words in one plot\n",
    "    max_words = 10\n",
    "    words = plot_preprocess(dictionary, max_words)[0]\n",
    "    counts = plot_preprocess(dictionary, max_words)[1]\n",
    "    \n",
    "    plt.bar(range(len(counts)), counts, align='center')\n",
    "    plt.xticks(range(len(words)), words)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do these graphs tell us anything about whether Shakespeare or Marlowe the more creative writer?\n",
    "How does the pulp author Edgar Rice Burrough fare?\n",
    "Are these results surprising to you, or is this all expected given Zipf's law?\n",
    "Or did we make mistakes in our analysis?\n",
    "If so, what changes should we make?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving to bigrams and trigrams\n",
    "\n",
    "One could argue that just counting words is not an adequate representation of a writer's style.\n",
    "Often the important thing is not so much which words an author uses, but how they use them.\n",
    "While a sophisicated analysis of writing style is a very difficult affair, our unigram approach can be easily tweaked to move beyond words.\n",
    "\n",
    "One way of formalizing the idea of *creative word usage* is to look in which order words appear.\n",
    "That is very easy with n-grams.\n",
    "From the list of tokenized words, one can easily construct a list of n-grams.\n",
    "For example, the list\n",
    "\n",
    "    [\"John\", \"likes\", \"John\", \",\", \"Bill\", \"'s\", \"mother\", \",\", \"and\", \"Sue\", \".\"]\n",
    "    \n",
    "could be converted to a list of bigrams.\n",
    "\n",
    "    [(\"John\", \"likes\"),\n",
    "     (\"likes\", \"John\"),\n",
    "     (\"John\", \",\"),\n",
    "     (\",\", \"Bill\"),\n",
    "     (\"Bill\", \"'s\"),\n",
    "     (\"'s\", \"mother\"),\n",
    "     (\",\", \"and\"),\n",
    "     (\"and\", \"Sue\"),\n",
    "     (\"Sue\", \".\")]\n",
    "\n",
    "Note that we treat n-grams as tuples in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_ngrams(tokenized_list, n):\n",
    "    \"\"\"convert tokenized list to list of n-grams (n = 2 or greater)\"\"\"\n",
    "    # some magic happens here\n",
    "    ngram_list = [tuple(tokenized_list[pos:pos+n])\n",
    "                  for pos in range(len(tokenized_list) - (n - 1))]\n",
    "    return ngram_list\n",
    "\n",
    "hamlet_bigrams = list_to_ngrams(hamlet, 4)\n",
    "faustus_bigrams = list_to_ngrams(faustus, 4)\n",
    "mars_bigrams = list_to_ngrams(mars, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can repeat our previous analysis using bigrams or trigrams instead of just words.\n",
    "(We could also look at 4-grams or 5-grams, but that is a lot harder to compute and we just don't have enough data to use such large n-grams effectively --- however, the 4-gram counts for Hamlet are somewhat surprising compared to the other works.)\n",
    "So let's see if the step from words to bigrams/trigrams changes things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute dictionaries to keep track of how often each n-gram occurs.\n",
    "# You do not need to worry about stop words in this case.\n",
    "\n",
    "hamlet_bigram_counts = ngram_counter(hamlet_bigrams)\n",
    "faustus_bigram_counts = ngram_counter(faustus_bigrams)\n",
    "mars_bigram_counts = ngram_counter(mars_bigrams)\n",
    "\n",
    "# Plot the data with matplotlib and see if the frequencies change a lot.\n",
    "for dictionary in [hamlet_bigram_counts, faustus_bigram_counts, mars_bigram_counts]:\n",
    "    # you can change the max words value to look at more or fewer words in one plot\n",
    "    max_words = 100\n",
    "    words = plot_preprocess(dictionary, max_words)[0]\n",
    "    counts = plot_preprocess(dictionary, max_words)[1]\n",
    "    \n",
    "    plt.bar(range(len(counts)), counts, align='center')\n",
    "    plt.xticks(range(len(words)), words)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now it is time for you to formulate a final verdict:\n",
    "\n",
    "- Has the move to bigrams/trigrams changed the results a lot?\n",
    "- Do those results reflect your personal expectations about the author's writing styles?\n",
    "- Depending on your answer, how much would you trust such quantiative approaches to evaluating writing style?\n",
    "- Would it surprise you to hear that more and more colleges across the US are relying on such software to grade student essays? Would you want your own essay to be graded this way?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
